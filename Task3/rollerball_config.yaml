# набор конфигураций для разных типов поведения агентов
behaviors:
  RollerBall:
    # Proximal Policy Optimization - это алгоритм обучения с подкреплением от OpenAI
    trainer_type: ppo
    # гиперпараметры
    hyperparameters:
      # Количество опытов на каждой итерации градиентного спуска.
      batch_size: 10
      # Количество опыта, который необходимо собрать перед обновлением модели политики. Соответствует тому, сколько опыта должно быть собрано, прежде чем мы приступим к какому-либо изучению или обновлению модели. 
      buffer_size: 100
      # Начальная скорость обучения для градиентного спуска. Соответствует силе каждого шага обновления градиентного спуска.
      learning_rate: 3.0e-4
      # Сила регуляризации энтропии, которая делает политику "более случайной". Это гарантирует, что агенты должным образом исследуют пространство действий во время обучения. 
      beta: 5.0e-4
      # Соответствует допустимому порогу расхождения между старой и новой политиками при обновлении с градиентным спуском. Установка этого значения небольшим приведет к более стабильным обновлениям, но также замедлит процесс обучения.
      epsilon: 0.2
      # насколько агент полагается на свою текущую оценку значений при расчете предсказаний. Высокие значения соответствуют тому, что агент больше полагается на фактические вознаграждения, полученные в окружающей среде
      lambd: 0.99
      # Количество проходов, которые необходимо выполнить через буфер опыта при выполнении оптимизации градиентного спуска.
      num_epoch: 3
      # Определяет, как скорость обучения меняется с течением времени.
      learning_rate_schedule: linear
    # настройки нейронной сети
    network_settings:
      # Применяется ли нормализация к входным данным векторного наблюдения. 
      normalize: false
      # Количество нейронов в скрытых слоях нейронной сети
      hidden_units: 128
      # Количество скрытых слоев в нейронной сети. Соответствует количеству скрытых слоев, присутствующих после входящих данных
      num_layers: 2
    # настройки для внешних и внутренних сигналов вознаграждения
    reward_signals:
      extrinsic:
        # Коэффициент дисконтирования для будущих вознаграждений, поступающих от окружающей среды. Это можно рассматривать как то, насколько далеко в будущем агент должен заботиться о возможных вознаграждениях.
        gamma: 0.99
        # Значение, на которое можно умножить вознаграждение, получаемое от окружающей среды
        strength: 1.0
    # общее количество шагов до завершения обучения
    max_steps: 500000
    # Сколько шагов опыта нужно собрать для каждого агента, прежде чем добавлять его в буфер опыта.
    time_horizon: 64
    # Количество опыта, которое необходимо собрать перед созданием и отображением статистики обучения.
    summary_freq: 10000
    # использовать мультипоточность
    threaded: true